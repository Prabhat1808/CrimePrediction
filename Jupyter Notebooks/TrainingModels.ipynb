{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainingModels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Mz_1lmhE8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4hUt1m5f7iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM and RNN code derived from the following github repo: https://github.com/TobiasLee/Text-Classification\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from tensorflow.contrib.rnn import BasicLSTMCell\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "import tensorflow as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rw4kUmTHw8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (365x78x4) matrices, as mentioned in the preprocessing steps\n",
        "\n",
        "input311File = 'matrices311'\n",
        "inputCrimeFile = 'matricesCR'\n",
        "\n",
        "with open(inputCrimeFile, 'rb') as pickle_file:\n",
        "    anomaly = pickle.load(pickle_file)\n",
        "\n",
        "with open(inputCrimeFile,'rb') as pickle_file:\n",
        "    content = pickle.load(pickle_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpLNLOnjYQxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing the crime and anomaly matrices\n",
        "# To remove the 0th row of each of the (78x4) matrices\n",
        "# ,since they contain data events for which geographical data was not available\n",
        "# as mentioned in the preprocessing code\n",
        "\n",
        "dat = []\n",
        "dat2 = []\n",
        "for i in range(len(content)):\n",
        "  a = []\n",
        "  b = []\n",
        "  for j in range(77):\n",
        "    a.extend(content[i][j+1])\n",
        "    b.extend(anomaly[i][j+1])\n",
        "    # print(a)\n",
        "  dat.append(a)\n",
        "  dat2.append(b)\n",
        "\n",
        "inp = np.array(dat)\n",
        "inp1 = np.where(inp>0,1,0)inp = np.array(dat)\n",
        "inp1 = np.where(inp>0,1,0)\n",
        "inpA = np.array(dat2)\n",
        "inpA = np.array(dat2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZbeSPW4cFX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train test split for the above data\n",
        "\n",
        "size = int(len(inp)*0.8)\n",
        "x_train = inp[:size]\n",
        "y_train = inp1[:size]\n",
        "x_test = inp[size:]\n",
        "y_test = inp1[size:]\n",
        "x_train2 = inpA[:size]\n",
        "x_test2 = inpA[size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gfz6cDfgQrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
        "    \"\"\"\n",
        "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
        "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
        "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
        "    Variables notation is also inherited from the article\n",
        "    Args:\n",
        "        inputs: The Attention inputs.\n",
        "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
        "                In case of RNN, this must be RNN outputs `Tensor`:\n",
        "                    If time_major == False (default), this must be a tensor of shape:\n",
        "                        `[batch_size, max_time, cell.output_size]`.\n",
        "                    If time_major == True, this must be a tensor of shape:\n",
        "                        `[max_time, batch_size, cell.output_size]`.\n",
        "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
        "                the backward RNN outputs `Tensor`.\n",
        "                    If time_major == False (default),\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
        "                    If time_major == True,\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
        "        attention_size: Linear size of the Attention weights.\n",
        "        time_major: The shape format of the `inputs` Tensors.\n",
        "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
        "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
        "            Using `time_major = True` is a bit more efficient because it avoids\n",
        "            transposes at the beginning and end of the RNN calculation.  However,\n",
        "            most TensorFlow data is batch-major, so by default this function\n",
        "            accepts input and emits output in batch-major form.\n",
        "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
        "            Used for visualization purpose.\n",
        "    Returns:\n",
        "        The Attention output `Tensor`.\n",
        "        In case of RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell.output_size]`.\n",
        "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(inputs, tuple):\n",
        "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
        "        inputs = tf.concat(inputs, 2)\n",
        "\n",
        "    if time_major:\n",
        "        # (T,B,D) => (B,T,D)\n",
        "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
        "\n",
        "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
        "\n",
        "    # Trainable parameters\n",
        "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "    with tf.name_scope('v'):\n",
        "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
        "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
        "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
        "\n",
        "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape\n",
        "\n",
        "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
        "\n",
        "    if not return_alphas:\n",
        "        return output\n",
        "    else:\n",
        "        return output, alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQDCuDUWgWQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def split_dataset(x_test, y_test, dev_ratio):\n",
        "    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n",
        "    test_size = len(x_test)\n",
        "    print(test_size)\n",
        "    dev_size = (int)(test_size * dev_ratio)\n",
        "    print(dev_size)\n",
        "    x_dev = x_test[:dev_size]\n",
        "    x_test = x_test[dev_size:]\n",
        "    y_dev = y_test[:dev_size]\n",
        "    y_test = y_test[dev_size:]\n",
        "    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n",
        "\n",
        "\n",
        "def fill_feed_dict(data_X, data_Y, batch_size):\n",
        "    \"\"\"Generator to yield batches\"\"\"\n",
        "    # Shuffle data first.\n",
        "    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n",
        "    # print(\"before shuffle: \", data_Y[:10])\n",
        "    # print(data_X.shape[0])\n",
        "    # perm = np.random.permutation(data_X.shape[0])\n",
        "    # data_X = data_X[perm]\n",
        "    # shuffled_Y = data_Y[perm]\n",
        "    # print(\"after shuffle: \", shuffled_Y[:10])\n",
        "    for idx in range(data_X.shape[0] // batch_size):\n",
        "        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n",
        "        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n",
        "        yield x_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZMRnuWSgf3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "MAX_DOCUMENT_LENGTH = 128\n",
        "EMBEDDING_SIZE = 128\n",
        "HIDDEN_SIZE = 64\n",
        "ATTENTION_SIZE = 64\n",
        "lr = 5e-4\n",
        "learning_rate=0.001\n",
        "hidden_dim = 250\n",
        "BATCH_SIZE = 4\n",
        "KEEP_PROB = 1.0\n",
        "LAMBDA = 0.0001\n",
        "MAX_LABEL = 77*4\n",
        "epochs = 10\n",
        "latent_dim = 8\n",
        "# n_batches = 1\n",
        "timeSize = 10\n",
        "max_len=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gABUU4alh137",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_label_hot(prediction, threshold=0.5):\n",
        "    prediction = tf.cast(prediction, tf.float32)\n",
        "    threshold = float(threshold)\n",
        "    return tf.cast(tf.greater(prediction, threshold), tf.int64)\n",
        "\n",
        "def get_metrics(labels_tensor, one_hot_prediction, num_classes):\n",
        "    metrics = {}\n",
        "    with tf.variable_scope(\"metrics\"):\n",
        "        for scope in [\"train\", \"val\"]:\n",
        "            with tf.variable_scope(scope):\n",
        "                with tf.variable_scope(\"accuracy\"):\n",
        "                    accuracy, accuracy_update = tf.metrics.accuracy(\n",
        "                        tf.cast(one_hot_prediction, tf.int32),\n",
        "                        labels_tensor,\n",
        "                    )\n",
        "                metrics[scope] = {\n",
        "                    \"accuracy\": accuracy,\n",
        "                    \"updates\": tf.group(accuracy_update),\n",
        "                }\n",
        "    return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTRzGFiLgmfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bi-LSTM based architecture with Attention\n",
        "# https://github.com/TobiasLee/Text-Classification\n",
        "\n",
        "tf.reset_default_graph()\n",
        "batch_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "anomaly_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "rnn_outputs1, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=batch_x, dtype=tf.float32,scope='BLSTM_1')\n",
        "fw_outputs1, bw_outputs1 = rnn_outputs1\n",
        "\n",
        "rnn_outputs2, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=anomaly_x, dtype=tf.float32,scope='BLSTM_2')\n",
        "fw_outputs2, bw_outputs2 = rnn_outputs2\n",
        "\n",
        "# weights for balance outs\n",
        "weight_out = tf.Variable(tf.truncated_normal([4], stddev=0.1))\n",
        "weight_soft = tf.nn.softmax(weight_out)\n",
        "\n",
        "inputAdd = weight_soft[0]*fw_outputs1 + weight_soft[1]**fw_outputs2 + weight_soft[2]*bw_outputs1 + weight_soft[3]*bw_outputs2\n",
        "print(batch_x.shape)\n",
        "print(inputAdd.shape)\n",
        "rnn_outputs, _ = bi_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=inputAdd, dtype=tf.float32,scope='BLSTM_3')\n",
        "fw_outputs, bw_outputs = rnn_outputs\n",
        "\n",
        "# # Attention\n",
        "# attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
        "# drop = tf.nn.dropout(attention_output, keep_prob)\n",
        "# shape = drop.get_shape()\n",
        "# print(shape)\n",
        "# # Fully connected layer（dense layer)\n",
        "# W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n",
        "# b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "# y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
        "W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=0.1))\n",
        "H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
        "M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
        "\n",
        "alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, HIDDEN_SIZE]),\n",
        "                                                tf.reshape(W, [-1, 1])),\n",
        "                                      (-1, timeSize )))  # batch_size x seq_len\n",
        "\n",
        "print(alpha.shape)\n",
        "r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
        "              tf.reshape(alpha, [-1, timeSize, 1]))\n",
        "r = tf.squeeze(r)\n",
        "h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
        "\n",
        "h_drop = tf.nn.dropout(h_star, keep_prob)\n",
        "shape = h_drop.get_shape()\n",
        "# print(h_star.shape)\n",
        "# Fully connected layer（dense layer)\n",
        "FC_W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=0.1))\n",
        "FC_b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat2 = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
        "print(y_hat2.shape)\n",
        "FC_W2 = tf.Variable(tf.truncated_normal([MAX_LABEL, MAX_LABEL], stddev=0.1))\n",
        "FC_b2 = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat = tf.nn.xw_plus_b(y_hat2, FC_W2, FC_b2)\n",
        "\n",
        "# ######## LOSS FUNCTIONS ######\n",
        "\n",
        "# This loss function is used to predict the actual number of crime occurences, hence the L2 loss\n",
        "loss =  tf.nn.l2_loss(y_hat-batch_y) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "# Uncomment this, if you just want the binary predictions, not actual crime numbers\n",
        "# loss =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y)) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "# ######## LOSS FUNCTIONS ######\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "\n",
        "# optimization\n",
        "# loss_to_minimize = loss\n",
        "# tvars = tf.trainable_variables()\n",
        "# gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "# grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n",
        "\n",
        "# global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "# optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "# train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step,\n",
        "#                                                 name='train_step')\n",
        "\n",
        "# Accuracy metric\n",
        "# prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n",
        "# accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, 1)), tf.float32))\n",
        "\n",
        "prediction = tf.sigmoid(y_hat)\n",
        "one_hot_prediction = multi_label_hot(prediction)\n",
        "\n",
        "accuracy  =  get_metrics(batch_y,one_hot_prediction,77)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNBTW_SUOX77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN based architecture with Attention\n",
        "# https://github.com/TobiasLee/Text-Classification\n",
        "\n",
        "tf.reset_default_graph()\n",
        "batch_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "anomaly_x = tf.placeholder(tf.float32, [None,timeSize,MAX_LABEL])\n",
        "batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "rnn_outputs1, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=batch_x, dtype=tf.float32,scope='BLSTM_1')\n",
        "\n",
        "rnn_outputs2, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=anomaly_x, dtype=tf.float32,scope='BLSTM_2')\n",
        "\n",
        "# weights for balance-outs\n",
        "weight_out = tf.Variable(tf.truncated_normal([2], stddev=0.1))\n",
        "weight_soft = tf.nn.softmax(weight_out)\n",
        "\n",
        "inputAdd = weight_soft[0]*rnn_outputs1 + weight_soft[1]*rnn_outputs2\n",
        "print(batch_x.shape)\n",
        "print(inputAdd.shape)\n",
        "rnn_outputs, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE),\n",
        "                        inputs=inputAdd, dtype=tf.float32,scope='BLSTM_3')\n",
        "# fw_outputs, bw_outputs = rnn_outputs\n",
        "\n",
        "\n",
        "# # Attention\n",
        "# attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
        "# drop = tf.nn.dropout(attention_output, keep_prob)\n",
        "# shape = drop.get_shape()\n",
        "# print(shape)\n",
        "# # Fully connected layer（dense layer)\n",
        "# W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n",
        "# b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "# y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
        "W = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=0.1))\n",
        "H = rnn_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n",
        "M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n",
        "\n",
        "alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, HIDDEN_SIZE]),\n",
        "                                                tf.reshape(W, [-1, 1])),\n",
        "                                      (-1, timeSize )))  # batch_size x seq_len\n",
        "\n",
        "print(alpha.shape)\n",
        "r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n",
        "              tf.reshape(alpha, [-1, timeSize, 1]))\n",
        "r = tf.squeeze(r)\n",
        "h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n",
        "\n",
        "h_drop = tf.nn.dropout(h_star, keep_prob)\n",
        "shape = h_drop.get_shape()\n",
        "# print(h_star.shape)\n",
        "# Fully connected layer（dense layer)\n",
        "FC_W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MAX_LABEL], stddev=0.1))\n",
        "FC_b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat2 = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n",
        "print(y_hat2.shape)\n",
        "FC_W2 = tf.Variable(tf.truncated_normal([MAX_LABEL, MAX_LABEL], stddev=0.1))\n",
        "FC_b2 = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n",
        "y_hat = tf.nn.xw_plus_b(y_hat2, FC_W2, FC_b2)\n",
        "\n",
        "loss =  tf.nn.l2_loss(y_hat-batch_y) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "# loss =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y)) +0.001*tf.nn.l2_loss(FC_W)+0.001*tf.nn.l2_loss(FC_W2) + 0.0001*tf.nn.l2_loss(W)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "prediction = tf.sigmoid(y_hat)\n",
        "one_hot_prediction = multi_label_hot(prediction)\n",
        "\n",
        "accuracy  =  get_metrics(batch_y,one_hot_prediction,77)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Uuo9D8Uxdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "!mkdir checkpointDir\n",
        "\n",
        "# Model Parameters\n",
        "slim = tf.contrib.slim\n",
        "sess=tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.local_variables_initializer())\n",
        "def model_summary():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "    \n",
        "model_summary()\n",
        "\n",
        "# To store training and test results for visualization\n",
        "\n",
        "tr = []\n",
        "ts = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYc8y8uIhCHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sess.run(tf.global_variables_initializer())\n",
        "print(\"Initialized! \")\n",
        "target_names = ['a','b','c','d']\n",
        "print(\"Start trainning\")\n",
        "start = time.time()\n",
        "\n",
        "testA = 0\n",
        "predsAr = []\n",
        "\n",
        "# Training for 50 epochs\n",
        "for e in range(50):\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    print(\"Epoch %d start !\" % (e + 1))\n",
        "    # for x_batch, y_batch in zip(x_train, y_train, BATCH_SIZE):\n",
        "    err = []\n",
        "    preds = []\n",
        "    trues = []\n",
        "\n",
        "    # batch sizes for crimes and anomaly and prediction\n",
        "    x_batch1 =[]\n",
        "    x_batch2 = []\n",
        "    y_batch1 = []\n",
        "    \n",
        "    # Recording error, prediction, truth values(for F1-scores)\n",
        "    for i in range(len(x_train)-80):\n",
        "        i+=80\n",
        "        x_batch = x_train[i:min(len(x_train)-1,timeSize+(i))]\n",
        "        x_anomaly = x_train2[i:min(len(x_train)-1,timeSize+(i))]\n",
        "        if len(x_batch) < timeSize:\n",
        "          continue\n",
        "        x_batch = x_batch\n",
        "        x_anomaly = x_anomaly\n",
        "        y_batch = x_train[min(len(x_train)-1,timeSize+(i))].T\n",
        "        x_batch1.append(x_batch)\n",
        "        x_batch2.append(x_anomaly)\n",
        "        y_batch1.append(y_batch)\n",
        "        if (i+1)% BATCH_SIZE >0:\n",
        "          continue\n",
        "        # print(np.array(x_batch1).shape)\n",
        "        fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "        # print(y_batch)\n",
        "        l, _, oht = sess.run([loss, optimizer, one_hot_prediction], feed_dict=fd)\n",
        "        for j in range(BATCH_SIZE):\n",
        "          # print(oht.shape)\n",
        "          preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "          trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "        x_batch1 =[]\n",
        "        y_batch1 = []\n",
        "        x_batch2 = []\n",
        "\n",
        "        # sess.run(optimizer,feed_dict=fd)\n",
        "        err.append(l)\n",
        "    # print(sess.run(loss))\n",
        "    epoch_finish = time.time()\n",
        "    # print(preds)\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "    # f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
        "    f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "    f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "    tr.append([f1,f2])\n",
        "    # print(f1)\n",
        "    print(\"TRain :: \",np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \",epoch_finish-epoch_start)\n",
        "    # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
        "\n",
        "    # Predictions on test data and storing info for visualization\n",
        "    if True:\n",
        "      preds = []\n",
        "      trues = []\n",
        "      x_batch1 =[]\n",
        "      y_batch1 = []\n",
        "      x_batch2 = []\n",
        "      err = []\n",
        "      for i in range(len(x_test)):\n",
        "          # i+=100\n",
        "          x_batch = x_test[i:min(len(x_test)-1,timeSize+(i))]\n",
        "          x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
        "          if len(x_batch) < timeSize:\n",
        "            continue\n",
        "          x_batch = x_batch\n",
        "          x_anomaly = x_anomaly\n",
        "          y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
        "          x_batch1.append(x_batch)\n",
        "          x_batch2.append(x_anomaly)\n",
        "          y_batch1.append(y_batch)\n",
        "          if (i+1)% BATCH_SIZE >0:\n",
        "            continue\n",
        "          fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "          l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
        "          err.append(l)\n",
        "          # sess.run(optimizer,feed_dict=fd)\n",
        "          for j in range(BATCH_SIZE):\n",
        "            # print(oht.shape)\n",
        "            preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "            trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "\n",
        "          x_batch1 =[]\n",
        "          y_batch1 = []\n",
        "          x_batch2 = []\n",
        "\n",
        "      # print(preds)\n",
        "      preds = np.array(preds)\n",
        "      trues = np.array(trues)\n",
        "      # f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
        "      f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "      f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "      if testA < f1:\n",
        "        testA=f1\n",
        "        save_path = saver.save(sess, \"./modelM/model\"+str(f1)[:5]+\".ckpt\")\n",
        "        # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
        "        predsAr.append(preds)\n",
        "      ts.append([f1,f2])\n",
        "      # print(f1)\n",
        "      print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \")\n",
        "      print(weightSupport)\n",
        "      # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXtCiDV5cvxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwcf4aSMwZIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This part of code is to visualize the decay in model performance as we try to predict crimes for an extended period of time, using bootstrapping\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "x_batch1 =[]\n",
        "y_batch1 = []\n",
        "x_batch2 = []\n",
        "err = []\n",
        "pp = []\n",
        "x_test3 = np.array(x_test)\n",
        "sub=0\n",
        "for i in range(len(x_test)):\n",
        "    # i+=100\n",
        "    i-=sub\n",
        "    x_batch = x_test3[i:min(len(x_test)-1,timeSize+(i))]\n",
        "    x_anomaly = x_test2[i:min(len(x_test)-1,timeSize+(i))]\n",
        "    if len(x_batch) < timeSize:\n",
        "      continue\n",
        "    x_batch = x_batch\n",
        "    x_anomaly = x_anomaly\n",
        "    y_batch = x_test[min(len(x_test)-1,timeSize+(i))].T\n",
        "    x_batch1.append(x_batch)\n",
        "    x_batch2.append(x_anomaly)\n",
        "    y_batch1.append(y_batch)\n",
        "    if (i+1)% BATCH_SIZE >0:\n",
        "      continue\n",
        "      sub=3\n",
        "    fd = {batch_x: x_batch1,anomaly_x:x_batch2, batch_y: y_batch1, keep_prob: KEEP_PROB}\n",
        "    l, acc,oht,weightSupport = sess.run([loss, accuracy,one_hot_prediction,weight_soft], feed_dict=fd)\n",
        "    err.append(l)\n",
        "    # sess.run(optimizer,feed_dict=fd)\n",
        "    for j in range(1):\n",
        "      # print(oht.shape)\n",
        "      preds.extend(np.array(oht[j]).reshape(-1,4))\n",
        "      trues.extend(np.array(y_batch1[j]).reshape(-1,4))\n",
        "    f1 = f1_score(y_true=np.where(np.array(trues)>0,1,0), y_pred=np.where(np.array(preds)>0,1,0), average='micro')\n",
        "    f2 = f1_score(y_true=np.where(np.array(trues)>0,1,0), y_pred=np.where(np.array(preds)>0,1,0), average='macro')\n",
        "    print(f1,\" : \",f2)\n",
        "    pp.append([f1,f2])\n",
        "\n",
        "    x_batch1 =[]\n",
        "    y_batch1 = []\n",
        "    x_batch2 = []\n",
        "    # x_test3[min(len(x_test)-1,timeSize+(i))]=np.array(oht[3]).T\n",
        "    # x_test3[min(len(x_test)-1,timeSize+(i-1))]=np.array(oht[2]).T\n",
        "    # x_test3[min(len(x_test)-1,timeSize+(i-2))]=np.array(oht[1]).T\n",
        "    x_test3[min(len(x_test)-1,timeSize+(i-3))]=np.array(oht[0]).T\n",
        "# print(preds)\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "# f1 = f1_score(y_true=y_batch, y_pred=oht, average='weighted')\n",
        "f1 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='micro')\n",
        "f2 = f1_score(y_true=np.where(trues>0,1,0), y_pred=np.where(preds>0,1,0), average='macro')\n",
        "# if testA < f1:\n",
        "  # testA=f1\n",
        "  # save_path = saver.save(sess, \"./modelM/model\"+str(f1)[:5]+\".ckpt\")\n",
        "  # print(classification_report(y_true=trues,y_pred=preds,target_names=target_names))\n",
        "  # predsAr.append(preds)\n",
        "ts.append([f1,f2])\n",
        "# print(f1)\n",
        "print(np.mean(err),\" : micro \",f1,\" : macro\",f2,\" : \")\n",
        "print(weightSupport)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZz9WfW54BF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting the above data\n",
        "\n",
        "%matplotlib inline\n",
        "plt.plot(np.array(pp).T[0].T,label=\"testProg_micro\")\n",
        "plt.plot(np.array(pp).T[1].T,label=\"testProg_macro\")\n",
        "# plt.plot(np.array(ts).T[0].T,label=\"test_micro\")\n",
        "# plt.plot(np.array(ts).T[1].T,label=\"test_macro\")\n",
        "# plt.plot(ts)\n",
        "plt.title('F1 fall progressive prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLGWbw8yss31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Statistical significance\n",
        "\n",
        "from scipy.stats import ttest_ind,ttest_rel,ks_2samp\n",
        "ttest_ind(predsAr[6].reshape(-1), predsAr[7].reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6o9L8l-LzBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seperating the model predictions to change the dimensions to (4 * days X 77) from (77 * days X 4)\n",
        "\n",
        "np.array(predsAr[8])\n",
        "a = []\n",
        "b = []\n",
        "i =0\n",
        "ar = []\n",
        "br= []\n",
        "for x,y in zip(predsAr[8],trues):\n",
        "  a.append(x)\n",
        "  b.append(y)\n",
        "  i+=1\n",
        "  if i%77==0:\n",
        "    ar.extend(list(np.array(a).T))\n",
        "    br.extend(list(np.array(b).T))\n",
        "    a = []\n",
        "    b = []\n",
        "\n",
        "br = np.array(br)\n",
        "ar = np.array(ar)\n",
        "\n",
        "print(classification_report(y_true=br,y_pred=ar))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL6tKdQuiC9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ks_2samp(predsAr[6].reshape(-1), predsAr[8].reshape(-1))\n",
        "# Statisticall significance test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGyITSa0l3Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification scores based on crime categories \n",
        "\n",
        "target_names = ['robery','burgalry','felony','grand']\n",
        "print(classification_report(y_true=trues,y_pred=predsAr[8],target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udPk5SfGnl1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting heatmap from crime vs locality, after reshaping for better representation\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig2, ax2 = plt.subplots()\n",
        "fig3, ax3 = plt.subplots()\n",
        "\n",
        "min_val, max_val = 0, 15\n",
        "\n",
        "intersection_matrix =  predsAr[8][-77*3:].reshape((11*3,14*2)) #np.random.randint(0, 10, size=(max_val, max_val))\n",
        "intersection_matrix2 = trues[-77*3:].reshape((11*3,14*2))\n",
        "ax.matshow(intersection_matrix, cmap=plt.cm.Blues)\n",
        "ax2.matshow(intersection_matrix2,  cmap=plt.cm.Greens)\n",
        "\n",
        "results = [[intersection_matrix2[i][j] + intersection_matrix[i][j]  for j in range\n",
        "(len(intersection_matrix2[0]))] for i in range(len(intersection_matrix2))]\n",
        "\n",
        "ax3.matshow(results,  cmap=plt.cm.Greens)\n",
        "# ax3.matshow(intersection_matrix,  cmap=plt.cm.Greens)\n",
        "# for i in range(14*2):\n",
        "#     for j in range(33):\n",
        "#         c = intersection_matrix[j,i]\n",
        "#         ax.text(i, j, str(c), va='center', ha='center')\n",
        "\n",
        "\n",
        "# for i in range(14*2):\n",
        "#     for j in range(33):\n",
        "#         c = intersection_matrix2[j,i]\n",
        "#         ax2.text(i, j, str(c), va='center', ha='center')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnEcwJTirHWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Heatmaps\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig2, ax2 = plt.subplots()\n",
        "fig3, ax3 = plt.subplots()\n",
        "\n",
        "min_val, max_val = 0, 15\n",
        "\n",
        "intersection_matrix =  predsAr[8][-77*2:].reshape((11*2,14*2)) #np.random.randint(0, 10, size=(max_val, max_val))\n",
        "intersection_matrix2 = trues[-77*2:].reshape((11*2,14*2))\n",
        "ax.matshow(intersection_matrix, cmap=plt.cm.Blues)\n",
        "ax2.matshow(intersection_matrix2,  cmap=plt.cm.Greens)\n",
        "\n",
        "results = [[intersection_matrix2[i][j] + intersection_matrix[i][j]  for j in range\n",
        "(len(intersection_matrix2[0]))] for i in range(len(intersection_matrix2))]\n",
        "\n",
        "ax3.matshow(results,  cmap=plt.cm.Greens)\n",
        "# ax3.matshow(intersection_matrix,  cmap=plt.cm.Greens)\n",
        "for i in range(14*2):\n",
        "    for j in range(33):\n",
        "        c = intersection_matrix[j,i]\n",
        "        ax.text(i, j, str(c), va='center', ha='center')\n",
        "\n",
        "\n",
        "for i in range(14*2):\n",
        "    for j in range(33):\n",
        "        c = intersection_matrix2[j,i]\n",
        "        ax2.text(i, j, str(c), va='center', ha='center')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}