{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POI_Segregation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN7em0OiLt2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UNCOMMENT IF USING COLAB\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_rm_PxMsea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shapely.wkt\n",
        "from shapely.geometry import Point, Polygon, MultiPolygon\n",
        "import re\n",
        "\n",
        "# Reads the point-of-interest dataset from New York City\n",
        "# Important locations, classified as following, with the geographical data and date of creation/updation to records\n",
        "\n",
        "# 1 Residential\n",
        "# 2 Education Facility\n",
        "# 3 Cultural Facility\n",
        "# 4 Recreational Facility\n",
        "# 5 Social Services\n",
        "# 6 Transportation Facility\n",
        "# 7 Commercial\n",
        "# 8 Government Facility (non public safety)\n",
        "# 9 Religious Institution\n",
        "# 10 Health Services\n",
        "# 11 Public Safety\n",
        "# 12 Water\n",
        "# 13 Miscellaneous\n",
        "\n",
        "filePOI = 'RawData/Point_Of_Interest.csv'\n",
        "fileRegions = \"RawData/Regions.csv\"\n",
        "\n",
        "\n",
        "poi = pd.read_csv(filePOI,low_memory=False)\n",
        "locations = pd.read_csv(fileRegions, low_memory=False)\n",
        "\n",
        "relevantColumns = ['the_geom','CREATED','FACILITY_T']\n",
        "finalColumns = ['Precincts','FACILITY_T']\n",
        "poi = poi[relevantColumns]\n",
        "\n",
        "# Reading geographical divisions of precincts to localize the POIs to their respective regions\n",
        "precincts = {}\n",
        "for index, row in locations.iterrows():\n",
        "  precincts[row['Precinct']] = shapely.wkt.loads(row['the_geom'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H21NZuyUNUdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting longitude and latitude details of various POIs\n",
        "# Extracting date of creation or updation to records for various POIs\n",
        "# This information is used later to localize each point\n",
        "\n",
        "lat = []\n",
        "longt = []\n",
        "date = []\n",
        "\n",
        "for idx,row in poi.iterrows():\n",
        "  location = re.split(r' |\\)|\\(' ,row['the_geom'])\n",
        "  year = int(row['CREATED'].split()[0].split('/')[-1])\n",
        "  lat = lat + [location[-2]]\n",
        "  longt = longt + [location[2]]\n",
        "  date = date + [year]\n",
        "\n",
        "poi['Year'] = date\n",
        "poi['Latitude'] = lat\n",
        "poi['Longitude'] = longt\n",
        "\n",
        "\n",
        "# Assigning the precinct number to a location in which the property is\n",
        "pos = 0\n",
        "prec = np.ndarray((poi.shape[0],))\n",
        "\n",
        "for index,row in poi.iterrows():\n",
        "\n",
        "  poo = Point(float(row['Longitude']),float(row['Latitude']))  \n",
        "  for key,val in precincts.items():\n",
        "    if poo.within(val):\n",
        "      prec[pos] = key\n",
        "      break\n",
        "\n",
        "  pos=pos+1\n",
        "  if(pos%1000 == 0):\n",
        "    print (\"processed \"+str(pos)+\" records!!\")\n",
        "    \n",
        "poi['Precincts'] = prec.astype(int)\n",
        "print(\"Done!!\")\n",
        "\n",
        "poi['Precincts'] = poi['Precincts'].astype(np.int64)\n",
        "poi = poi[poi['Precincts'] >= 0]\n",
        "\n",
        "\n",
        "# Inverted dictionaries for easy conversion of the above dataframe to a matrix that is usable later in the neural architecture\n",
        "\n",
        "# For categories of the locality, listed above\n",
        "inv_categories = {}\n",
        "n_categories = poi['FACILITY_T'].nunique()\n",
        "uniq_categories = poi['FACILITY_T'].unique()\n",
        "for i in range(n_categories):\n",
        "  inv_categories[uniq_categories[i]] = i\n",
        "\n",
        "# For geographical division of precincts\n",
        "n_precincts = poi['Precincts'].nunique()\n",
        "uniq_precincts = poi['Precincts'].unique()\n",
        "inv_prec = {}\n",
        "for i in range(n_precincts):\n",
        "  inv_prec[uniq_precincts[i]] = i\n",
        "\n",
        "poi = poi[poi['Year'] <= 2008]\n",
        "poi = poi[finalColumns]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t05lKhiLrTkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrices = np.zeros((n_precincts,n_categories),dtype=np.int64)\n",
        "exceptions = 0\n",
        "\n",
        "for idx, row in poi.iterrows():\n",
        "  try:\n",
        "    id1 = inv_prec[row['Precincts']]\n",
        "    id2 = inv_categories[row['FACILITY_T']]\n",
        "    matrices[id1][id2]= matrices[id1][id2] + 1\n",
        "  except:\n",
        "    print(\"Exception!!!\")\n",
        "    print(\"Precincts\",id1)\n",
        "    print(\"FACILITY_T\",id2)\n",
        "    exceptions = exceptions + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyLegbV1mZGq",
        "colab_type": "code",
        "outputId": "5d4011d9-677d-4a1b-c9c8-605204cde743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import pickle\n",
        "\n",
        "outputPOI = 'poiMatrices'\n",
        "\n",
        "# Pickle dump this file\n",
        "file = open(outputPOI'wb')\n",
        "pickle.dump(matrices,file)\n",
        "\n",
        "# To load this files, use:\n",
        "# with open(filename,'rb') as toOpen:\n",
        "#     data = pickle.load(toOpen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "311Preprocessing.ipynb\tCrimePreprocessing.ipynb  POI_Segregation.ipynb\n",
            "anomalyMatrices\t\tFile\t\t\t  RawData\n",
            "Archives\t\tMergeDatasets.ipynb\t  Resources\n",
            "crimeMatrices\t\tpoiMatrices\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}